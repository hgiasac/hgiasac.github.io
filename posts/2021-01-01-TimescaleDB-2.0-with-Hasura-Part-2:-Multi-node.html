<html><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><title>TimescaleDB 2.0 with Hasura Part 2: Multi node (2021-01-01)</title><meta property="og:title" content="TimescaleDB 2.0 with Hasura Part 2: Multi node (2021-01-01)"/><meta property="twitter:title" content="TimescaleDB 2.0 with Hasura Part 2: Multi node (2021-01-01)"/><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,500"/><link rel="stylesheet" href="/assets/main.min.css"/><link rel="stylesheet" href="https://maxcdn.icons8.com/fonts/line-awesome/1.1/css/line-awesome-font-awesome.min.css"/></head><meta name="description" content=""/><meta property="og:description" content=""/><meta property="twitter:description" content=""/><body><header><div class="container grid-lg"><nav class="horizontal main-nav"><ul><li><a href="/">Home</a></li><li><a href="/posts">Posts</a></li><li><a href="/about.html">About</a></li><li><a href="https://www.visualcv.com/toan-nguyen">CV</a></li></ul></nav></div></header><section class="section"><div id="main"><div class="wrapper"><div class="container grid-lg"><hr class="mt-2"/><article class="article"><h1 id="TimescaleDB_20_with_Hasura_Part_2_-_Multi-node">TimescaleDB 2.0 with Hasura Part 2 - Multi-node</h1><p><img src="/assets/timescale-hasura.png" alt="Hasura TimescaleDB"/></p><p>Multi-node is the most interesting feature of version 2.0 that provides the ability to create a cluster of TimescaleDB instances to scale both reads and writes. A multi-node TimescaleDB cluster consists of:</p><ul><li>One access node to handle ingest, data routing and act as an entry point for user access;</li><li>One or more data nodes to store and organize distributed data.</li></ul><p>In this post, I will try setting up this Multi-node feature and run with Hasura.</p><blockquote><p>This is final part of the series:</p><ul><li><a href="/posts/2020-12-31-TimescaleDB-2.0-with-Hasura-Part-1:-From-1.x-to-2.0.html">Part 1 - From 1.x to 2.0</a></li><li><a href="/posts/2021-01-01-TimescaleDB-2.0-with-Hasura-Part-2:-Multi-node.html">Part 2 - Multi-node</a></li><li><a href="/posts/2021-01-02-TimescaleDB-2.0-with-Hasura-Part-3:-High-Availability.html">Part 3 - High availability</a></li></ul></blockquote><blockquote><p>The example code is uploaded on <a href="https://github.com/hgiasac/hasura-timescaledb2-example">Github</a></p></blockquote><h2 id="Infrastructure_setup">Infrastructure setup</h2><p><img src="/assets/timescale-multinode.png" alt="TimescaleDB Multi-node diagram"/></p><p>Cluster requirements:</p><ul><li>Network infrastructure is ready. The access node have to ensure that it is able to connect to data nodes when adding them.</li><li>Authentication between nodes. The access node distributes requests to data nodes through client library, so it still need authentication credential. </li></ul><p>There are 3 authentication mechanisms:</p><ul><li>Trust authentication<ul><li>Password authentication</li><li>Certificate authentication</li></ul></li></ul><p>In this demo, we will use first mechanism. You can think it is insecure. I don&#39;t disagree. However, it will be fine if we place TimescaleDB cluster into private cloud, and expose access node to internet only. Docker environment can emulate it.</p><p>*You can read more detail about authentication in <a href="https://docs.timescale.com/latest/getting-started/setup-multi-node-basic/setup-multi-node-auth##node-communication">official TimescaleDB docs</a>*</p><p>First of all, we need to register data nodes to the access node. </p><pre class="code" data-lang="sql"><code>SELECT add_data_node(&#39;data1&#39;, host =&gt; &#39;timescaledb-data1&#39;);</code></pre><p><code>host</code> can be IP or DNS name. In docker, you can use alias name. </p><blockquote><p>Note: this function requires running in <code>AUTOCOMMIT</code> mode. So you can only run it with <code>psql</code>, or custom migration CLI. However, I don&#39;t recommend using migration, because Multi-node clusters configuration is different if you run multiple development environments (dev, staging, production...)</p></blockquote><p>You can add nodes manually with <code>psql</code>. Fortunately the Postgres docker image supports initialization hooks <code>&#x2F;docker-entrypoint-initdb.d</code>. You can use bash script to add them automatically.</p><pre class="code" data-lang="sh"><code># multinode&#x2F;scripts&#x2F;add-data-nodes.sh<br/>psql -v ON_ERROR_STOP=1 -U &quot;$POSTGRES_USER&quot; -d &quot;$POSTGRES_DB&quot; &lt;&lt;-EOSQL<br/>  SELECT add_data_node(&#39;$node&#39;, host =&gt; &#39;$node&#39;);<br/>EOSQL</code></pre><blockquote><p>Can we register reversely from data node to access node?</p></blockquote><p>It is possible. However you have to customize Postgres image. The temporary Postgres server for initialization scripts isn&#39;t exposed to external network (<a href="https://github.com/docker-library/postgres/blob/03e769531fff4c97cb755e4a608b24935ceeee27/docker-entrypoint.sh#L238">source</a>). Therefore we can&#39;t run remote <code>psql</code> to register node.</p><p>Another solutions are:</p><ul><li>Creating a initialization container that waits all Postgres server online, then run <code>psql</code> script.</li><li>Patroni with <code>postInit</code> hooks. </li></ul><h2 id="Create_Distributed_hypertable">Create Distributed hypertable</h2><p>Next step, we need to create distributed hypertable instead of normal, non-distributed hypertable. The difference of Distributed hypertable is, the data is stored across data node instances. The access node stores metadata of data nodes and distributes the requests and queries appropriately to those nodes, then aggregates the results received from them. Non-distributed <code>hypertable</code> still stores data locally in current server.</p><p>The function is similar to <code>hypertable</code>. We run it after creating table</p><pre class="code" data-lang="sql"><code>CREATE TABLE conditions (<br/>  time        TIMESTAMPTZ       NOT NULL DEFAULT NOW(),<br/>  location    TEXT              NOT NULL,<br/>  temperature DOUBLE PRECISION  NULL,<br/>  humidity    DOUBLE PRECISION  NULL<br/>);<br/><br/>SELECT create_distributed_hypertable(&#39;conditions&#39;, &#39;time&#39;);</code></pre><p>Here we encounter another issue. </p><p><img src="/assets/timescale-create-distribution-hypertable-error.png" alt="Create Distribution Hypertable Error"/></p><p>Prepared transactions are disabled by default. <code>timescaledb-tune</code> doesn&#39;t automatically enable this setting too. We have to set it manually.</p><pre class="code" data-lang="ini"><code>max_prepared_transactions = 150<br/>enable_partitionwise_aggregate = on</code></pre><p>You can mount manual <code>postgres.conf</code> file in Docker container. However, it can&#39;t take advantage of <code>timescaledb-tune</code>. Fortunately the Postgres imae also supports config flags. You can set it as arguments.</p><pre class="code" data-lang="yaml"><code>timescaledb-data1:<br/>  image: timescale&#x2F;timescaledb:2.0.0-pg12<br/>  command:<br/>    - &quot;-cmax_prepared_transactions=150&quot;<br/>  # ...</code></pre><p>Restart services and run the function again. You can verify by querying hypertable information:</p><pre class="code" data-lang="sql"><code>select * from timescaledb_information.hypertables;</code></pre><p>| hypertable_schema | hypertable_name | owner | num_dimensions | num_chunks | compression_enabled | is_distributed | replication_factor | data_nodes | tablespaces |
| ----------------- | --------------- | ----- | -------------- |------------| ------------------- | -------------- | ------------------ | ---------- | ----------- |
| public | conditions | postgres | 1 | 2 | f | t | 1 | {timescaledb-data1,timescaledb-data2} | |</p><p>You may notice in <code>is_distributed</code> column that represent whether this hypertable is distributed or not. On data nodes, this column is <code>false</code> or non-distributed. That means if the hypertable is detached, it can work as normal hypertable. </p><h2 id="Work_with_data">Work with data</h2><p>In this test, I will run SQL script to insert 1 million rows. Let&#39;s see how long the execution takes and number of rows per data node.</p><pre class="code" data-lang="sql"><code>INSERT INTO conditions<br/>    SELECT time, &#39;US&#39;, (random()*30)::int, random()*80<br/>    FROM generate_series(<br/>      &#39;2020-01-01 00:00:00&#39;::timestamptz, <br/>      &#39;2020-01-01 00:00:00&#39;::timestamptz + INTERVAL &#39;999999 seconds&#39;, <br/>      &#39;1 second&#39;)<br/>    AS time;<br/><br/>Planning Time: 1.297 ms<br/>Execution Time: 11763.459 ms</code></pre><p>| access | timescaledb-data1 | timescaledb-data2 |
| ------ | ----------------- | ----------------- |
| 1,000,000 | 395,200 | 604,800 |</p><p>The distribution ratio isn&#39;t usually 1:1. The variance is larger when we insert large number of concurrent requests.
The insert speed is very fast. It takes about 11 seconds to insert 1 million row. In contrast, UPDATE and DELETE operations take very long time.
The below table show performance comparison between non-distributed hypertable and distributed hypertable, in milliseconds.</p><p>| Operation | Hypertable | Hypertable (JIT) | Distributed hypertable | Distributed hypertable (JIT) |
| --------- | ---------- | ---------------- | ---------------------- | ---------------------------- |
| INSERT | 3,707.195 | 4,586.120 | 12,700.597 | 11,763.459 |<br/>| UPDATE | 4,353.752 | 4,404.471 | 184,707.671 | 204,340.519 |
| SELECT | 174.012 | 210.479 | 2,011.194 | 1,057.094 |
| DELETE | 737.954 | 883.095 | 159,165.419 | 184,945.111 |</p><ul><li>Distributed hypertable performance is much worse than non-distributed one. In my opinion, the access node has to distribute requests to data nodes through network. This causes higher latency cost than inserting directly into disk. Moreover, the access node take more computing power to query from all data nodes, then aggregate the final result.</li><li>UPDATE and DELETE operations are extremely slow. We should avoid modifying data.</li><li>JIT mode isn&#39;t really boost performance. From above table, the performance is slightly improved on Distributed hypertable, but slower on non-distributed one.</li></ul><p>However, the main purpose of distributed hypertable is handling more write requests. The slower performance is expected trade-off.</p><h2 id="Foreign_key_and_Relationship">Foreign key and Relationship</h2><p>For example, we create a table <code>locations</code>. <code>conditions</code> table reference <code>locations</code> through <code>location</code> column.</p><pre class="code" data-lang="sql"><code>CREATE TABLE &quot;public&quot;.&quot;locations&quot;(<br/>  &quot;id&quot; Text NOT NULL, <br/>  &quot;description&quot; text NOT NULL, <br/>  PRIMARY KEY (&quot;id&quot;) <br/>);<br/><br/>ALTER TABLE conditions ADD CONSTRAINT conditions_locations_fk <br/>  FOREIGN KEY (location) REFERENCES locations (id);<br/>-- ERROR:  [timescaledb-data1]: relation &quot;locations&quot; does not exist</code></pre><p>The foreign key can&#39;t be created. Because <code>locations</code> table is available in the access node only. Data nodes don&#39;t know about it.
However, <code>JOIN</code> query is worked on the access node. TimescaleDB is smart enough to map relational data.</p><pre class="code" data-lang="sql"><code>SELECT * FROM conditions JOIN locations ON conditions.location = locations.id;<br/><br/>             time              | location | temperature | humidity | id | description  <br/>-------------------------------+----------+-------------+----------+----+--------------<br/> 2020-12-31 16:32:42.328627+00 | US       |          10 |        1 | US | United State<br/> 2020-12-31 16:34:01.648958+00 | US       |          12 |        1 | US | United State<br/> 2020-12-31 16:34:35.241304+00 | US       |          13 |       13 | US | United State</code></pre><h2 id="Continuous_Aggregate_View">Continuous Aggregate View</h2><p>We can&#39;t create Continuous Materialized view (see <a href="#caveats">Caveats</a>)</p><pre class="code" data-lang="log"><code>postgres= CREATE MATERIALIZED VIEW conditions_summary_minutely<br/>postgres-     WITH (timescaledb.continuous) AS<br/>postgres-     SELECT time_bucket(INTERVAL &#39;1 minute&#39;, time) AS bucket,<br/>postgres-            AVG(temperature),<br/>postgres-            MAX(temperature),<br/>postgres-            MIN(temperature)<br/>postgres-     FROM conditions<br/>postgres-     GROUP BY bucket;<br/>ERROR:  continuous aggregates not supported on distributed hypertables</code></pre><p>Fortunately we still can create View or Materialized view. Therefore we can do workaround with View or the combination of Materialized view and scheduled job. The downside is worse performance than the continuous view.</p><pre class="code" data-lang="sql"><code>CREATE MATERIALIZED VIEW conditions_summary_hourly  AS<br/>    SELECT time_bucket(INTERVAL &#39;1h&#39;, time) AS bucket,<br/>           AVG(temperature),<br/>           MAX(temperature),<br/>           MIN(temperature)<br/>    FROM conditions<br/>    GROUP BY bucket;<br/>    <br/>CREATE OR REPLACE PROCEDURE conditions_summary_hourly_refresh(job_id int, config jsonb) LANGUAGE PLPGSQL AS<br/>$$<br/>BEGIN<br/>  REFRESH MATERIALIZED VIEW conditions_summary_hourly;     <br/>END<br/>$$;<br/><br/>SELECT add_job(&#39;conditions_summary_hourly_refresh&#39;,&#39;1h&#39;);</code></pre><h2 id="SQL_function">SQL function</h2><p>Custom SQL functions for GraphQL queries work well. These functions are creating on Access node only. However, the query engine is smart enough to get data from data nodes.</p><pre class="code" data-lang="sql"><code>CREATE OR REPLACE FUNCTION search_conditions(location text) <br/>    RETURNS SETOF conditions <br/>    LANGUAGE sql STABLE<br/>AS $$<br/>    SELECT * FROM conditions <br/>        WHERE conditions.location = location;<br/>$$</code></pre><p>However, Trigger doesn&#39;t work. The access node doesn&#39;t automatically create SQL functions and Triggers to data nodes. You have to create them on each data node. However it isn&#39;t good way to do.</p><p>There are more limitations that you can see in <a href="#caveats">Caveats</a> section.</p><h2 id="Run_with_Hasura_GraphQL_Engine">Run with Hasura GraphQL Engine</h2><p>It isn&#39;t different from original Postgres. GraphQL Engine service only needs connecting to the access node&#39;s database URL. There are still issues that are similar to non-distributed <code>hypertable</code>. View, SQL query function works as expected.</p><p><img src="/assets/timescale-hasura-console-data.png" alt="Hasura Graphql Engine Console data"/></p><p><img src="/assets/timescale-hasura-console-graphql.png" alt="Hasura Graphql Engine Console Query"/></p><p>However, because Foreign key doesn&#39;t work on distributed hypertable, GraphQL Engine can&#39;t automatically suggest relationship. You have to define manually.</p><p><img src="/assets/timescale-hasura-relationship-create.png" alt="Manual Relationship"/></p><p><img src="/assets/timescale-hasura-relationship-query.png" alt="Relationship Query"/></p><h2 id="Caveats">Caveats</h2><p>Beside non-distribution hypertable caveats, distributed hypertable has more downsides:</p><ul><li>Besides hypertable, another features aren&#39;t distributed: background jobs, Continuous aggregate view, compression policies, reordering chunks...</li><li>Joins on data nodes are not supported.</li><li>Consistent database structure required between access and data nodes.</li><li>If you create SQL functions, foreign keys,... you have to create them in all data nodes. The access node can&#39;t automatically do it for you. </li><li>Native replication is still in experiment. </li></ul><p>The root cause is in the access node that can&#39;t manage consistent metadata with data nodes yet. It can&#39;t automatically sync database structure to data node except distributed hypertables. However, TimescaleDB 2.0 is still in early stage. The caveats still have chance to be solved in next versions.</p><p>You can read more detail in <a href="https://docs.timescale.com/latest/using-timescaledb/limitations#distributed-hypertable-limitations">TimescaleDB docs</a></p><h2 id="So_should_I_use_it">So, should I use it?</h2><p>Yes, if you really need to scale write performance, and don&#39;t use Continuous aggregate view, compression policies features, at least in near future. In theory, you still can create them manually in each data node, since these hypertables in data nodes are normal one. However this workaround can lead to inconsistent database structures between nodes.</p><p>The access node is still a TimescaleDB server. You still can create tables and local hypertables and use them along with distributed hypertables.</p></article></div></div></div></section><footer><div class="wrapper"><div class="container grid-lg"><hr/><div class="text-center"><a class="secondary" href="https://github.com/hgiasac"><i class="fa fa-github fa-2x"> </i></a><a class="secondary" href="https://www.linkedin.com/in/toan-nguyen-83295527/"><i class="fa fa-linkedin fa-2x"> </i></a><a class="secondary" href="https://stackoverflow.com/users/4230985/hgiasac"><i class="fa fa-stack-overflow fa-2x"> </i></a></div><div class="text-center mt-2">Â© Toan Nguyen 2017-2021</div></div></div></footer></body></html>