<html><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><title>TimescaleDB 2.0 with Hasura Part 3: High Availability (2021-01-02)</title><meta property="og:title" content="TimescaleDB 2.0 with Hasura Part 3: High Availability (2021-01-02)"/><meta property="twitter:title" content="TimescaleDB 2.0 with Hasura Part 3: High Availability (2021-01-02)"/><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,500"/><link rel="stylesheet" href="/assets/main.min.css"/><link rel="stylesheet" href="https://maxcdn.icons8.com/fonts/line-awesome/1.1/css/line-awesome-font-awesome.min.css"/></head><meta name="description" content=""/><meta property="og:description" content=""/><meta property="twitter:description" content=""/><body><header><div class="container grid-lg"><nav class="horizontal main-nav"><ul><li><a href="/">Home</a></li><li><a href="/posts">Posts</a></li><li><a href="/about.html">About</a></li><li><a href="https://www.visualcv.com/toan-nguyen">CV</a></li></ul></nav></div></header><section class="section"><div id="main"><div class="wrapper"><div class="container grid-lg"><hr class="mt-2"/><article class="article"><h1 id="TimescaleDB_20_with_Hasura_Part_3_-_High_Availability">TimescaleDB 2.0 with Hasura Part 3 - High Availability</h1><p><img src="/assets/timescale-hasura.png" alt="Hasura TimescaleDB"/></p><p> Performance and High availability are critical challenges in production. Although TimescaleDB solves performance problems, we still need to care about remain parts. In this article, I suggest some high availability setup for single-node as well as explore multi-node replication solutions,</p><blockquote><p>This is final part of the series:</p><ul><li><a href="/posts/2020-12-31-TimescaleDB-2.0-with-Hasura-Part-1:-From-1.x-to-2.0.html">Part 1 - From 1.x to 2.0</a></li><li><a href="/posts/2021-01-01-TimescaleDB-2.0-with-Hasura-Part-2:-Multi-node.html">Part 2 - Multi-node</a></li><li><a href="/posts/2021-01-02-TimescaleDB-2.0-with-Hasura-Part-3:-High-Availability.html">Part 3 - High availability</a></li></ul></blockquote><blockquote><p>The example code is uploaded on <a href="https://github.com/hgiasac/hasura-timescaledb2-example">Github</a></p></blockquote><h2 id="Single-node">Single-node</h2><p>TimescaleDB is an extension of Postgres, so you can use any replicated solution of Postgres. The common solution is streaming replication, combine with <a href="https://repmgr.org/">repmgr</a> or <a href="https://patroni.readthedocs.io/en/latest">Patroni</a> for automatic failover.</p><p>If you use Kubernetes, you can install <a href="https://github.com/timescale/timescaledb-kubernetes">official helm repository of TimescaleDB</a> that use Patroni with 3 nodes by default.</p><p><img src="/assets/timescale-create-distribution-hypertable-error.png" alt="TimescaleDB single node Kubernetes diagram"/>
TimescaleDB single node Kubernetes diagram. Source: https:&#x2F;&#x2F;github.com&#x2F;timescale&#x2F;timescaledb-kubernetes</p><h2 id="Multi-node">Multi-node</h2><p>Now we have multi-node cluster. However does it support high availability? If one data node is down, does TimescaleDB cluster still work?</p><p>With this test case, I stop data node <code>timescaledb-data1</code>, then test several operations:</p><ul><li>INSERT works. The access node can know status of data nodes , and distribute to alive nodes.</li><li>However SELECT, UPDATE and DELETE operations are failed. </li></ul><pre class="code" data-lang="sql"><code>postgres= INSERT INTO conditions VALUES (NOW(), &#39;US&#39;, 13, 1.3);<br/>INSERT 0 1<br/>postgres= SELECT * FROM conditions ORDER BY time DESC LIMIT 1;<br/>ERROR:  could not connect to &quot;timescaledb-data1&quot;<br/>DETAIL:  could not translate host name &quot;timescaledb-data1&quot; to address: Name does not resolve</code></pre><p>So, it only support high availability on INSERT. The query planner isn&#39;t smart enough to exclude outage data nodes. Therefore, you have to ensure high availability for all data nodes.</p><p>I came up with an idea. Can we temporarily detach outage data node? No, it can&#39;t, even with <code>force</code> option.</p><pre class="code" data-lang="sql"><code>postgres= SELECT detach_data_node(&#39;timescaledb-data1&#39;, force =&gt; true);<br/>ERROR:  insufficient number of data nodes<br/>DETAIL:  Distributed hypertable &quot;conditions&quot; would lose data if data node &quot;timescaledb-data1&quot; is detached.<br/>HINT:  Ensure all chunks on the data node are fully replicated before detaching it.</code></pre><h2 id="Native_Replication">Native Replication</h2><p>TimescaleDB 2.0 provides built-in, native replication support for multi-node. This feature is promising although it is still in development preview.
We aren&#39;t required any additional setup to use native replication. Just need to set <code>replication_factor</code> argument with integer value. This argument represents the number of data nodes that the same data is written to. The value must be in between 1 and total data nodes. The default value is 1. </p><p>Let&#39;s see how many rows are inserted into 2 data nodes:</p><p>| access | timescaledb-data1 | timescaledb-data2 |
| ------ | ----------------- | ----------------- |
| 1,000,000 | 1,000,000 | 1,000,000 |</p><p>From above table, the number of rows are equal. That means the data is copied into both data nodes. This ensures the consistency of data. The trade off is slower performance. Data nodes take more work to replicate data. The delay depends on the number of replication factors.</p><p>| Operation | replication_factor = 1 (ms) | replication_factor = 2 (ms) |
| --------- | --------------------------- | --------------------------- |
| INSERT | 12,700.597 | 16,083.890 |<br/>| UPDATE | 184,707.671 | 259,930.983 |
| SELECT | 2,011.194 | 2,023.870 |
| DELETE | 159,165.419 | 248,658.194 |</p><p>Surprisingly the <code>SELECT</code> performance is comparable with non-replication mode. The query planner knows how to include only one replica of each chunk in the query plan.
However, the outage behavior isn&#39;t correct as I thought. The <code>SELECT</code> query still throws error after stopping one data node.</p><pre class="code" data-lang="sql"><code>postgres= SELECT COUNT(*) FROM conditions;<br/>ERROR:  could not connect to &quot;timescaledb-data1&quot;           <br/>DETAIL:  could not translate host name &quot;timescaledb-data1&quot; to address: Name does not resolve</code></pre><p>Fortunately we can detach the data node now, with <code>force</code>.</p><pre class="code" data-lang="sql"><code>-- Still error if you don&#39;t force detachment<br/>postgres= SELECT detach_data_node(&#39;timescaledb-data1&#39;);<br/>ERROR:  data node &quot;timescaledb-data1&quot; still holds data for distributed hypertable &quot;conditions&quot;<br/><br/>postgres= SELECT detach_data_node(&#39;timescaledb-data1&#39;, force =&gt; true);<br/>WARNING:  distributed hypertable &quot;conditions&quot; is under-replicated<br/>DETAIL:  Some chunks no longer meet the replication target after detaching data node &quot;timescaledb-data1&quot;.<br/>WARNING:  insufficient number of data nodes for distributed hypertable &quot;conditions&quot;<br/>DETAIL:  Reducing the number of available data nodes on distributed hypertable &quot;conditions&quot; prevents full replication of new chunks.<br/><br/>-- the SELECT query works now<br/>postgres= select COUNT(*) FROM conditions;<br/><br/>  count    <br/>---------<br/> 1000000</code></pre><p>Unfortunately, we can&#39;t attach existing table from data node. What will we do? Drop the table in the data node and attach again? It
isn&#39;t a good choice.</p><pre class="code" data-lang="sql"><code>postgres= SELECT attach_data_node(&#39;timescaledb-data1&#39;, &#39;conditions&#39;, if_not_attached =&gt; true);<br/>ERROR:  [timescaledb-data1]: relation &quot;conditions&quot; already exists</code></pre><h2 id="Conclusion">Conclusion</h2><p>Up to now, the best choice for high availability solution for TimescaleDB multi-node feature is streaming replication. For ideal design, every node has 1 replica with failover support. </p><p><img src="/assets/timescale-multinode-replication.png" alt="TimescaleDB multi-node replication"/></p><p>Native replication feature is still in active development. It has many issues that need to be improved. At least the query planer should be smarter enough to when to ignore failure data nodes when reading data. Anyway, the replication solution is cool and worth to look forward.</p></article></div></div></div></section><footer><div class="wrapper"><div class="container grid-lg"><hr/><div class="text-center"><a class="secondary" href="https://github.com/hgiasac"><i class="fa fa-github fa-2x"> </i></a><a class="secondary" href="https://www.linkedin.com/in/toan-nguyen-83295527/"><i class="fa fa-linkedin fa-2x"> </i></a><a class="secondary" href="https://stackoverflow.com/users/4230985/hgiasac"><i class="fa fa-stack-overflow fa-2x"> </i></a></div><div class="text-center mt-2">Â© Toan Nguyen 2017-2021</div></div></div></footer></body></html>